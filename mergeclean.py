# merge_playlists.py
import os
import re
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

from tqdm import tqdm

from utils.filter_keywords import Indicators_key, Category_Key, Nsfw_Key
from config.sources_urls import playlist_urls
from utils.network import fetch_playlist_content, is_url_accessible
from utils.m3u_parse import parse_m3u

# --- é…ç½®åŒº ---
#EPG_URL = "https://tvpass.org/epg.xml"
EPG_URL="http://epg.51zmt.top:8000/e.xml"
OUTPUT_FILE = "out/MergedCleanPlaylist.m3u8"

# æ˜¯å¦å¯¹é¢‘é“è¿›è¡Œç­›é€‰, æ ¹æ®utils.filter_keywords.Category_Key
CategoryFilter = False

# å¹¶å‘æ£€æŸ¥URLæœ‰æ•ˆæ€§ ---
URL_CHECK = True

# å¹¶å‘æ£€æŸ¥URLæ—¶çš„æœ€å¤§çº¿ç¨‹æ•°ï¼Œå¯ä»¥æ ¹æ®ä½ çš„ç½‘ç»œå’ŒCPUæƒ…å†µè°ƒæ•´
MAX_WORKERS_URL_CHECK = 100


def is_nsfw(group_title, title):
    """æ£€æŸ¥é¢‘é“çš„ group-title æˆ– title æ˜¯å¦åŒ…å« NSFW å…³é”®è¯ã€‚"""
    nsfw_keywords = Nsfw_Key
    text_to_check = f"{group_title} {title}".lower()
    return any(keyword in text_to_check for keyword in nsfw_keywords)


def normalize_title(title):
    """
    ç²¾ç¡®åŒ¹é…æ›¿æ¢
    :param title: æ–‡æœ¬
    :return: æ–‡æœ¬
    """
    indicators = Indicators_key
    normalized = title
    for indicator in indicators:
        normalized = re.sub(indicator, '', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'[\s\-_|(\[\]]+$', '', normalized).strip()
    normalized = ' '.join(normalized.split())
    return normalized if normalized else title


def check_urls_concurrently(channels_to_check):
    """
    ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘æ£€æŸ¥é¢‘é“URLçš„å¯è®¿é—®æ€§ã€‚

    Args:
        channels_to_check (list): å¾…æ£€æŸ¥çš„é¢‘é“åˆ—è¡¨ã€‚

    Returns:
        list: åŒ…å«æ‰€æœ‰å¯è®¿é—®é¢‘é“çš„åˆ—è¡¨ã€‚
    """
    print(
        f"\nğŸš€ Starting concurrent URL accessibility check for {len(channels_to_check)} channels (up to {MAX_WORKERS_URL_CHECK} workers)...")
    accessible_channels = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS_URL_CHECK) as executor:
        # åˆ›å»º future åˆ° channel_data çš„æ˜ å°„
        # channel å…ƒç»„ç»“æ„: (..., headers, url)
        # headers åœ¨å€’æ•°ç¬¬2ä¸ªä½ç½® (channel[-2]), url åœ¨æœ€å (channel[-1])
        future_to_channel = {
            executor.submit(is_url_accessible, channel[-1], channel[-2], 15): channel
            for channel in channels_to_check
        }

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for future in tqdm(as_completed(future_to_channel), total=len(channels_to_check), desc="Checking URLs"):
            channel_data = future_to_channel[future]
            try:
                if future.result():
                    accessible_channels.append(channel_data)
            except Exception:
                # å‘ç”Ÿä»»ä½•å¼‚å¸¸ï¼ˆå¦‚è¶…æ—¶ï¼‰éƒ½è®¤ä¸ºURLä¸å¯è®¿é—®
                pass

    inaccessible_count = len(channels_to_check) - len(accessible_channels)
    print(f"âœ“ Accessible channels: {len(accessible_channels)}")
    if inaccessible_count > 0:
        print(f"âœ— Inaccessible or timed-out channels: {inaccessible_count}")
    return accessible_channels


def process_and_normalize_channels(accessible_channels):
    """
    å¯¹é¢‘é“åˆ—è¡¨è¿›è¡Œè§„èŒƒåŒ–ã€å»é‡å’Œç»Ÿä¸€åŒ–å¤„ç†ã€‚
    - è¿‡æ»¤ NSFW å†…å®¹å’ŒéæŒ‡å®šåˆ†ç±»ã€‚
    - æ ¹æ®å…³é”®å­— è¿›è¡Œåˆ†ç±»è¿‡æ»¤
    - è¿‡æ»¤ url å®Œå…¨é‡å¤çš„æ¡ç›®ã€‚
    - è§„èŒƒåŒ–é¢‘é“æ ‡é¢˜ã€‚
    - ç»Ÿä¸€åŒåé¢‘é“çš„ TVG ä¿¡æ¯ã€‚
    """
    print("\nğŸ” Starting data normalization, de-duplication, and unification...")

    processed_urls = set()
    master_tvg_info = {}
    final_channels = []
    filtered_count = 0

    for tvg_name, tvg_id, tvg_logo, group_title, title, headers, url in tqdm(accessible_channels,
                                                                             desc="Processing & Unifying"):
        # æ£€æŸ¥æ˜¯å¦ä¸º NSFW å†…å®¹
        if is_nsfw(group_title, title):
            filtered_count += 1
            continue

        # åˆ†ç±»è¿‡æ»¤
        if CategoryFilter:
            lower_keywords = [k.lower() for k in Category_Key]
            searchable_text = f'{tvg_name}, {group_title}, {title}'.lower()
            if not any(keyword in searchable_text for keyword in lower_keywords):
                filtered_count += 1
                continue

        # è¿‡æ»¤urlå®Œå…¨é‡å¤çš„æ¡ç›®
        if url in processed_urls:
            filtered_count += 1
            continue
        processed_urls.add(url)

        # è§„èŒƒåŒ–æ ‡é¢˜
        # normalized_title = normalize_title(title.strip())
        key = title.lower()

        # æ£€æŸ¥å¹¶ç»Ÿä¸€ TVG ä¿¡æ¯  å°†tvg_name = title ä»¥ç¬¦åˆèŠ‚ç›®å•æ˜¾ç¤ºé€»è¾‘  ä¿ç•™tvg_id ä»¥å¯¹èŠ‚ç›®å•è¿›è¡Œç­›é€‰
        if key not in master_tvg_info:
            master_tvg_info[key] = (tvg_logo, group_title)

        master_tvg_logo, master_group_title= master_tvg_info[key]

        # ä½¿ç”¨ç»Ÿä¸€åçš„ä¿¡æ¯æ„å»ºæœ€ç»ˆçš„é¢‘é“æ•°æ®
        unified_channel = (
           title, tvg_id, master_tvg_logo, master_group_title, title, headers, url
        )
        final_channels.append(unified_channel)

    if filtered_count > 0:
        print(f"ğŸš« Filtered out {filtered_count} channels based on keywords.")
    print(f"âœ… Kept {len(final_channels)} channels after processing.")
    return final_channels


def write_merged_playlist(final_channels_to_write):
    """å°†æœ€ç»ˆçš„é¢‘é“åˆ—è¡¨å†™å…¥ M3U æ–‡ä»¶ã€‚"""
    lines = [f'#EXTM3U url-tvg="{EPG_URL}"', ""]
    # æŒ‰ group-title å’Œ title æ’åº
    sorted_channels = sorted(
        final_channels_to_write,
        key=lambda channel: (str(channel[3]).lower(), str(channel[4]).lower())
    )

    current_group = None
    for channel_data in sorted_channels:
        tvg_name, tvg_id, tvg_logo, group, title, headers, url = channel_data

        if group != current_group:
            if current_group is not None:
                lines.append("")
            lines.append(f'#EXTGRP:{group}')
            current_group = group

        extinf_parts = ['#EXTINF:-1']
        if tvg_id: extinf_parts.append(f'tvg-id="{tvg_id}"')
            
        if tvg_name: 
            extinf_parts.append(f'tvg-name="{tvg_name}"')
        else:
            extinf_parts.append(f'tvg-name="{title}"')
            
        if tvg_logo: extinf_parts.append(f'tvg-logo="{tvg_logo}"')
        if group: extinf_parts.append(f'group-title="{group}"')

        extinf_line = ' '.join(extinf_parts) + f',{title}'
        lines.append(extinf_line)
        lines.extend(headers)
        lines.append(url)

    final_output_string = '\n'.join(lines) + '\n'
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(final_output_string)
    print(f"\nâœ… Merged playlist written to {OUTPUT_FILE}.")
    print(f"ğŸ“Š Total channels written: {len(final_channels_to_write)}.")


def main():
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    start_time = datetime.now()
    print(f"ğŸš€ Starting playlist merge at {start_time.strftime('%Y-%m-%d %H:%M:%S')}...")

    all_channels = []
    for url in playlist_urls:
        content = fetch_playlist_content(url)
        if content:
            parsed_channels = parse_m3u(content)
            print(f"âœ… Parsed {len(parsed_channels)} valid channel entries from {url}.")
            all_channels.extend(parsed_channels)

    if URL_CHECK:
        all_channels = check_urls_concurrently(all_channels)

    # --- ä¼˜åŒ–æ­¥éª¤ï¼šåªå¤„ç†å¯è®¿é—®çš„é¢‘é“ ---
    processed_channels = process_and_normalize_channels(all_channels)
    write_merged_playlist(processed_channels)

    end_time = datetime.now()
    print(f"\nâœ¨ Merging complete at {end_time.strftime('%Y-%m-%d %H:%M:%S')}.")
    print(f"â±ï¸ Total execution time: {(end_time - start_time).total_seconds():.2f} seconds.")


if __name__ == "__main__":
    main()















