import requests
import re
import time
from datetime import datetime
import concurrent.futures
from tqdm import tqdm
import os

# --- é…ç½®åŒº ---
# æ’­æ”¾åˆ—è¡¨æº
playlist_urls = [
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/DaddyLive.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/DaddyLiveEvents.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/DrewAll.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/JapanTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/PlexTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/PlutoTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/TubiTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/DrewLiveVOD.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/TVPass.m3u",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/Radio.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/Roku.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/TheTVApp.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/LGTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/AriaPlus.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/LocalNowTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/PPVLand.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/SamsungTVPlus.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/main/Xumo.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/FSTV24.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/MoveOnJoy.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/A1x.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/StreamedSU.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/SportsWebcast.m3u8"
]

# EPG ç”µå­èŠ‚ç›®å•åœ°å€
EPG_URL = "http://drewlive24.duckdns.org:8081/merged2_epg.xml.gz"
# è¾“å‡ºæ–‡ä»¶å
OUTPUT_FILE = "out/MergedCleanPlaylist.m3u8"
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

# --- åŠŸèƒ½å¼€å…³ ---
# æ˜¯å¦å¼€å¯ URL å¯ç”¨æ€§æ£€æµ‹ (ä¼šæ˜¾è‘—å¢åŠ è¿è¡Œæ—¶é—´)
CHECK_URLS = True
# æ£€æµ‹æ—¶çš„è¶…æ—¶æ—¶é—´ (ç§’)
URL_TIMEOUT = 5
# æ£€æµ‹æ—¶ä½¿ç”¨çš„æœ€å¤§çº¿ç¨‹æ•°
MAX_WORKERS = 20


def fetch_playlist(url, retries=3, timeout=15):
    """è·å–å¹¶è¿”å›æ’­æ”¾åˆ—è¡¨å†…å®¹"""
    headers = {"User-Agent": "Mozilla/5.0"}
    for attempt in range(1, retries + 1):
        try:
            print(f"Attempting to fetch {url} (try {attempt})...")
            res = requests.get(url, timeout=timeout, headers=headers)
            res.raise_for_status()
            print(f"âœ… Successfully fetched {url}")
            return res.text.strip().splitlines()
        except Exception as e:
            print(f"âŒ Attempt {attempt} failed for {url}: {e}")
            time.sleep(2)
    print(f"âš ï¸ Skipping {url} after {retries} failed attempts.")
    return []


def parse_playlist(lines, source_url="Unknown"):
    """
    ä¸€ä¸ªå¥å£®çš„M3Uè§£æå™¨ï¼Œä½¿ç”¨çŠ¶æ€æœºæ¨¡å‹å¤„ç†é¢‘é“å’Œåˆ†ç»„ã€‚
    - æ­£ç¡®å¤„ç† #EXTGRP ä¸Šä¸‹æ–‡ã€‚
    - åªæœ‰å½“ä¸€ä¸ªé¢‘é“åŒæ—¶æ‹¥æœ‰ #EXTINF å’Œ URL æ—¶æ‰è¢«è§†ä¸ºæœ‰æ•ˆã€‚
    - è‡ªåŠ¨ä¸ºç¼ºå°‘ group-title çš„é¢‘é“è¡¥å……åˆ†ç»„ä¿¡æ¯ã€‚
    """
    channels = []
    current_group = "Other"  # é»˜è®¤åˆ†ç»„

    # ä¸´æ—¶å­˜å‚¨å½“å‰æ­£åœ¨è§£æçš„é¢‘é“ä¿¡æ¯
    extinf = None
    headers = []

    for line in lines:
        line = line.strip()
        if not line:
            continue

        if line.startswith('#EXTGRP:'):
            # 1. é‡åˆ°æ–°çš„åˆ†ç»„å®šä¹‰
            current_group = line.split(':', 1)[-1].strip()
            # ä¸€ä¸ªæ–°çš„åˆ†ç»„å¼€å§‹ï¼Œæ„å‘³ç€ä¸Šä¸€ä¸ªä¸å®Œæ•´çš„é¢‘é“ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰åº”è¯¥è¢«ä¸¢å¼ƒ
            extinf = None
            headers = []

        elif line.startswith('#EXTINF:'):
            # 2. é‡åˆ°æ–°çš„é¢‘é“ä¿¡æ¯è¡Œ
            # å¦‚æœä¹‹å‰æœ‰ä¸€ä¸ªå¾…å¤„ç†çš„é¢‘é“ä½†æ²¡æœ‰URLï¼Œå®ƒå°†è¢«è¿™ä¸ªæ–°çš„#EXTINFè¦†ç›–ï¼ˆå³ä¸¢å¼ƒï¼‰
            extinf = line
            headers = []  # é‡ç½®å¤´éƒ¨ä¿¡æ¯

        elif line.startswith('#') and extinf:
            # 3. é‡åˆ°å…¶ä»–å¤´éƒ¨ä¿¡æ¯ï¼ˆå¦‚ #EXTVLCOPTï¼‰ï¼Œä¸”æˆ‘ä»¬æ­£å¤„äºä¸€ä¸ªé¢‘é“å—ä¸­
            headers.append(line)

        elif extinf and not line.startswith('#'):
            # 4. é‡åˆ°ä¸€ä¸ªé'#'å¼€å¤´çš„è¡Œï¼Œè¿™åº”è¯¥æ˜¯URL
            url_line = line

            # æ£€æŸ¥ #EXTINF è¡Œæ˜¯å¦å·²æœ‰ group-title
            group_title_match = re.search(r'group-title="([^"]+)"', extinf)

            if not group_title_match:
                # å¦‚æœ #EXTINF ä¸­æ²¡æœ‰ group-titleï¼Œä½¿ç”¨æˆ‘ä»¬ä» #EXTGRP è¿½è¸ªçš„å½“å‰åˆ†ç»„
                # å°è¯•åœ¨æœ€åä¸€ä¸ªå¼•å·åæ³¨å…¥ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒç¨³å¦¥çš„ä½ç½®
                new_extinf, count = re.subn(r'(")(?!.*")', rf'\1 group-title="{current_group}"', extinf,
                                            count=1)
                if count == 0:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¼•å·ï¼Œå°±ç›´æ¥è¿½åŠ 
                    new_extinf = f'{extinf} group-title="{current_group}"'
            else:
                # å¦‚æœ #EXTINF ä¸­å·²æœ‰ group-titleï¼Œåˆ™ä½¿ç”¨å®ƒè‡ªå·±çš„
                new_extinf = extinf

            # æ·»åŠ å®Œæ•´é¢‘é“è®°å½•
            channels.append((new_extinf, tuple(headers), url_line))

            # é‡ç½®çŠ¶æ€ï¼Œå‡†å¤‡è§£æä¸‹ä¸€ä¸ªé¢‘é“
            extinf = None
            headers = []

    print(f"âœ… Parsed {len(channels)} valid channel entries from {source_url}.")
    return channels


def normalize_title(title):
    """
    ç§»é™¤é¢‘é“åç§°ä¸­çš„æ¸…æ™°åº¦ã€æ¥æºç­‰æ ‡è¯†ï¼Œå®ç°åŒååŒ–ã€‚
    ä¾‹å¦‚: 'ESPN HD' -> 'ESPN', 'Fox Sports 501 FHD' -> 'Fox Sports 501'
    """
    # å®šä¹‰è¦ç§»é™¤çš„å…³é”®è¯åˆ—è¡¨ï¼Œ\bç¡®ä¿åŒ¹é…çš„æ˜¯å®Œæ•´å•è¯
    indicators = [
        r'\bFHD\b', r'\bHD\b', r'\bSD\b', r'\bUHD\b', r'\b4K\b', r'\b2K\b', r'\b8K\b',

    ]

    normalized = title
    for indicator in indicators:
        normalized = re.sub(indicator, '', normalized, flags=re.IGNORECASE)

    # æ¸…ç†å¯èƒ½ç•™ä¸‹çš„å¤šä½™ç©ºæ ¼ã€æœ«å°¾çš„è¿å­—ç¬¦æˆ–æ‹¬å·
    normalized = re.sub(r'[\s\-_|(\[\]]+$', '', normalized).strip()
    # å°†å¤šä¸ªè¿ç»­ç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    normalized = ' '.join(normalized.split())

    return normalized if normalized else title


def process_and_normalize_channels(all_channels_list):
    """
    æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼š
    1. åŸºäºURLè¿›è¡Œç²¾ç¡®å»é‡ã€‚
    2. å¯¹é¢‘é“åç§°è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚
    """
    print("\nğŸ” Starting normalization and de-duplication process...")

    processed_urls = set()
    final_channels = []

    for extinf, headers, url in tqdm(all_channels_list, desc="Processing Channels"):
        # 1. URLå»é‡ï¼šå¦‚æœè¿™ä¸ªæµåœ°å€å·²ç»å¤„ç†è¿‡ï¼Œå°±è·³è¿‡
        if url in processed_urls:
            continue
        processed_urls.add(url)

        # 2. åç§°æ ‡å‡†åŒ–
        try:
            info_part, original_title = extinf.rsplit(',', 1)
            original_title = original_title.strip()
        except ValueError:
            # è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„ #EXTINF è¡Œ
            continue

        normalized_display_title = normalize_title(original_title)

        # é‡æ–°æ„å»º #EXTINF è¡Œï¼Œåªæ›´æ–°æœ«å°¾çš„æ˜¾ç¤ºåç§°
        new_extinf = f"{info_part},{normalized_display_title}"

        final_channels.append((new_extinf, headers, url))

    print(f"âœ… Kept {len(final_channels)} unique channels after processing.")
    return final_channels


def is_nsfw(extinf, headers, url):
    """æ£€æŸ¥é¢‘é“æ¡ç›®æ˜¯å¦åŒ…å«NSFWå…³é”®è¯"""
    nsfw_keywords = ['nsfw', 'xxx', 'porn', 'adult']
    combined_text = f"{extinf.lower()} {' '.join(headers).lower()} {url.lower()}"
    group_match = re.search(r'group-title="([^"]+)"', extinf.lower())
    if group_match and any(k in group_match.group(1) for k in nsfw_keywords):
        return True
    return any(k in combined_text for k in nsfw_keywords)


def is_url_accessible(channel_data):
    """
    æ£€æŸ¥å•ä¸ªURLæ˜¯å¦å¯è®¿é—®ã€‚
    å¦‚æœå¯è®¿é—®ï¼Œè¿”å›åŸå§‹é¢‘é“æ•°æ®ï¼›å¦åˆ™è¿”å›Noneã€‚
    """
    extinf, headers, url = channel_data
    try:
        response = requests.head(url, timeout=URL_TIMEOUT, allow_redirects=True, headers={"User-Agent": "Mozilla/5.0"})
        if 200 <= response.status_code < 400:
            return channel_data
    except (requests.exceptions.Timeout, requests.exceptions.RequestException):
        pass
    return None


def check_channel_urls(channels_to_check):
    """ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œæ£€æŸ¥æ‰€æœ‰é¢‘é“çš„URLå¯ç”¨æ€§ã€‚"""
    accessible_channels = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(is_url_accessible, data): data for data in channels_to_check}

        for future in tqdm(concurrent.futures.as_completed(future_to_url), total=len(channels_to_check),
                           desc="Checking URLs"):
            result = future.result()
            if result:
                accessible_channels.append(result)

    return accessible_channels


def write_merged_playlist(final_channels_to_write):
    """å°†æœ€ç»ˆçš„é¢‘é“åˆ—è¡¨æ’åºå¹¶å†™å…¥æ–‡ä»¶"""
    lines = [f'#EXTM3U url-tvg="{EPG_URL}"', ""]
    sortable_channels = []

    for extinf, headers, url in final_channels_to_write:
        group_match = re.search(r'group-title="([^"]+)"', extinf)
        group = group_match.group(1) if group_match else "Other"
        try:
            # ä½¿ç”¨æˆ‘ä»¬å·²ç»æ ‡å‡†åŒ–è¿‡çš„åç§°è¿›è¡Œæ’åº
            title = extinf.rsplit(',', 1)[-1].strip()
        except IndexError:
            title = "Unknown Title"
        sortable_channels.append((group.lower(), title.lower(), extinf, headers, url))

    sorted_channels = sorted(sortable_channels)
    current_group = None
    total_channels_written = 0

    for _, _, extinf, headers, url in sorted_channels:
        group_match = re.search(r'group-title="([^"]+)"', extinf)
        actual_group_name = group_match.group(1) if group_match else "Other"

        if actual_group_name != current_group:
            if current_group is not None:
                lines.append("")
            lines.append(f'#EXTGRP:{actual_group_name}')
            current_group = actual_group_name

        lines.append(extinf)
        lines.extend(headers)
        lines.append(url)
        total_channels_written += 1

    if lines and lines[-1] == "":
        lines.pop()

    final_output_string = '\n'.join(lines) + '\n'

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(final_output_string)

    print(f"\nâœ… Merged playlist written to {OUTPUT_FILE}.")
    print(f"ğŸ“Š Total channels written: {total_channels_written}.")
    print(f"ğŸ“ Total lines in output file: {len(final_output_string.splitlines())}.")


if __name__ == "__main__":
    start_time = time.time()
    print(f"ğŸš€ Starting playlist merge at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...")

    # 1. è·å–æ‰€æœ‰æºçš„åŸå§‹é¢‘é“æ•°æ®
    raw_channels_list = []
    for url in playlist_urls:
        lines = fetch_playlist(url)
        if lines:
            parsed_channels = parse_playlist(lines, source_url=url)
            raw_channels_list.extend(parsed_channels)

    # 2. ç»Ÿä¸€å¤„ç†ï¼šURLå»é‡å’Œåç§°æ ‡å‡†åŒ–
    processed_channels = process_and_normalize_channels(raw_channels_list)

    # 3. è¿‡æ»¤NSFWå†…å®¹
    non_nsfw_channels = [entry for entry in processed_channels if not is_nsfw(*entry)]
    removed_nsfw_count = len(processed_channels) - len(non_nsfw_channels)
    if removed_nsfw_count > 0:
        print(f"ğŸ—‘ï¸ Filtered out {removed_nsfw_count} NSFW channels.")

    # 4. å¯é€‰çš„URLå¯ç”¨æ€§æ£€æµ‹
    if CHECK_URLS:
        print("\nğŸŒ Starting URL accessibility check (this may take a while)...")
        final_list_to_write = check_channel_urls(non_nsfw_channels)
        inaccessible_count = len(non_nsfw_channels) - len(final_list_to_write)
        print(f"\nğŸ‘ Found {len(final_list_to_write)} accessible channels.")
        if inaccessible_count > 0:
            print(f"ğŸ—‘ï¸ Removed {inaccessible_count} inaccessible or timed-out channels.")
    else:
        print("\nâš ï¸ URL accessibility check is disabled. Skipping.")
        final_list_to_write = non_nsfw_channels

    # 5. å†™å…¥æœ€ç»ˆæ–‡ä»¶
    write_merged_playlist(final_list_to_write)

    end_time = time.time()
    print(f"\nâœ¨ Merging complete at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}.")

    print(f"â±ï¸ Total execution time: {end_time - start_time:.2f} seconds.")



